{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3WUb8t2P2e5"
      },
      "source": [
        "üá≠ üá™ üá± üá± üá¥ üëã\n",
        "\n",
        "This example shows how to use `segmentation-models-pytorch` for **binary** semantic segmentation. We will use the [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) (this is an adopted example from Albumentations package [docs](https://albumentations.ai/docs/examples/pytorch_semantic_segmentation/), which is strongly recommended to read, especially if you never used this package for augmentations before). \n",
        "\n",
        "The task will be to classify each pixel of an input image either as pet üê∂üê± or as a background.\n",
        "\n",
        "\n",
        "What we are going to overview in this example:  \n",
        "\n",
        " - üìú `Datasets` and `DataLoaders` preparation (with predefined dataset class).  \n",
        " - üì¶ `LightningModule` preparation: defining training, validation and test routines.  \n",
        " - üìà Writing `IoU` metric inside the `LightningModule` for measuring quality of segmentation.  \n",
        " - üê∂ Results visualization.\n",
        "\n",
        "\n",
        "> It is expected you are familiar with Python, PyTorch and have some experience with training neural networks before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYNdz8s56qOu",
        "outputId": "fc419f1c-92d8-46e9-f3dd-86cfaffcb1af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (4.65.0)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (0.15.1+cu118)\n",
            "Collecting timm==0.6.12\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from segmentation-models-pytorch) (8.4.0)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0+cu118)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm==0.6.12->segmentation-models-pytorch) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.22.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.25.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm==0.6.12->segmentation-models-pytorch) (23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16444 sha256=98c7efa6b9b838088b0ee76ed16f86254590ef0a4ad2242bbced91c308e1c3ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/16/24/752e89d88d333af39a288421e64d613b5f652918e39ef1f8e3\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60962 sha256=0a231d81c41c2542ac705e6b686e8296330e24975668d4ae427d7034cd78d9e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/3b/4e/2f3015f1ab76f34be28e04c4bcee27e8cabfa70d2eadf8bc3b\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, huggingface-hub, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 huggingface-hub-0.13.4 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.2 timm-0.6.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning==1.9.3\n",
            "  Downloading pytorch_lightning-1.9.3-py3-none-any.whl (826 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m826.4/826.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.3) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.3) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.3) (2.0.0+cu118)\n",
            "Collecting lightning-utilities>=0.6.0.post0\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.3) (2023.3.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.3) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.3) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.3) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.3) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.3) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.3) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.3) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.3) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9.3) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9.3) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9.3) (3.25.2)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.3) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.3) (22.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->pytorch-lightning==1.9.3) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.3) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.3) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.3) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning==1.9.3) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-1.9.3 torchmetrics-0.11.4 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install pytorch-lightning==1.9.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iKiMzw2t6ika"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import segmentation_models_pytorch as smp\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4RKHF535Twz"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkghwALE5fIc"
      },
      "source": [
        "In this example we will use predefined `Dataset` class for simplicity. The dataset actually read pairs of images and masks from disk and return `sample` - dictionary with keys `image`, `mask` and others (not relevant for this example).\n",
        "\n",
        "‚ö†Ô∏è **Dataset preparation checklist** ‚ö†Ô∏è\n",
        "\n",
        "In case you writing your own dataset, please, make sure that:\n",
        "\n",
        "1.   **Images** üñº  \n",
        "    ‚úÖ   Images from dataset have **the same size**, required for packing images to a batch.  \n",
        "    ‚úÖ   Images height and width are **divisible by 32**. This step is important for segmentation, because almost all models have skip-connections between encoder and decoder and all encoders have 5 downsampling stages (2 ^ 5 = 32). Very likely you will face with error when model will try to concatenate encoder and decoder features if height or width is not divisible by 32.  \n",
        "    ‚úÖ   Images have **correct axes order**. PyTorch works with CHW order, we read images in HWC [height, width, channels], don`t forget to transpose image.\n",
        "2.   **Masks** üî≥  \n",
        "    ‚úÖ   Masks have **the same sizes** as images.   \n",
        "    ‚úÖ   Masks have only `0` - background and `1` - target class values (for binary segmentation).  \n",
        "    ‚úÖ   Even if mask don`t have channels, you need it. Convert each mask from **HW to 1HW** format for binary segmentation (expand the first dimension).\n",
        "\n",
        "Some of these checks are included in LightningModule below during the training.\n",
        "\n",
        "‚ùóÔ∏è And the main rule: your train, validation and test sets are not intersects with each other!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_7MB3RVCtmF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP_DttTvvyQN"
      },
      "outputs": [],
      "source": [
        "# from segmentation_models_pytorch.datasets import SimpleOxfordPetDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVHVkntIS6Cr"
      },
      "outputs": [],
      "source": [
        "# # download data\n",
        "# root = \".\"\n",
        "# SimpleOxfordPetDataset.download(root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmBUxn6shipI"
      },
      "outputs": [],
      "source": [
        "# def my_segmentation_transforms(image, segmentation):\n",
        "#     if random.random() > 0.5:\n",
        "#         angle = random.randint(-30, 30)\n",
        "#         image = F.rotate(image, angle)\n",
        "#         segmentation = F.rotate(segmentation, angle)\n",
        "#     # more transforms ...\n",
        "#     return image, segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1hPTqJcRwH_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CVCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,root, filenames_before, mode=\"train\", transform=None):\n",
        "\n",
        "        assert mode in {\"train\", \"valid\", \"test\"}\n",
        "\n",
        "        self.root = root\n",
        "        self.filenames_before=filenames_before\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images_directory = os.path.join(self.root, \"images\")\n",
        "        self.masks_directory = os.path.join(self.root, \"annotations\")\n",
        "\n",
        "        self.filenames = self._read_split()  # read train/valid/test splits\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        filename = self.filenames[idx]\n",
        "        # image_path = os.path.join(self.images_directory, filename ) # path to images\n",
        "        # mask_path = os.path.join(self.masks_directory, filename )   # path to annotations\n",
        "        self.root1 = \"/content/drive/MyDrive/4-2/ML project/CVC-dataset/\"\n",
        "        if filename.endswith(\"modified\"):\n",
        "           image_path = os.path.join(self.root1,\"PNG_EXTRA\",\"images\", filename + \".png\") # path to images\n",
        "           mask_path = os.path.join(self.root1,\"PNG_EXTRA\",\"annotations\", filename + \".png\")\n",
        "        else:\n",
        "           image_path = os.path.join(self.root1,\"PNG\",\"images\", filename + \".png\") # path to images\n",
        "           mask_path = os.path.join(self.root1,\"PNG\",\"annotations\", filename + \".png\")\n",
        "\n",
        "        # trimap = np.array(Image.open(mask_path))\n",
        "        # mask = self._preprocess_mask(trimap)\n",
        "        # sample = dict(image=image, mask=mask, trimap=trimap)\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        mask  = Image.open(mask_path).convert(\"L\")\n",
        "        \n",
        "\n",
        "        image = F.adjust_brightness(image,brightness_factor= 1.2)\n",
        "        # image = F.adjust_hue(image,hue_factor= -0.5)\n",
        "        image = F.adjust_saturation(image,saturation_factor=1.2)\n",
        "        image = F.adjust_contrast(image,contrast_factor=1.2)\n",
        "        image = F.adjust_sharpness(image,sharpness_factor=1.7)\n",
        "       \n",
        "\n",
        "        image = np.array(image)\n",
        "        mask  = np.array(mask)\n",
        "       \n",
        "\n",
        "      \n",
        "        # mask = np.array(Image.open(mask_path).convert(\"RGB\"))/255\n",
        "        \n",
        "        # print(\"before \\t\\t\\t\\t\\t\", image.shape)\n",
        "        # image , mask = my_segmentation_transforms(image,mask)\n",
        "        # print(\"after \\t\\t\\t\\t\\t\",image.shape)\n",
        "        # mask = Image.open(mask_path)\n",
        "\n",
        "        sample = dict(image=image, mask=mask)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(**sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    @staticmethod\n",
        "    def _preprocess_mask(mask):\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask[mask == 2.0] = 0.0\n",
        "        mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n",
        "        return mask\n",
        "\n",
        "    def _read_split(self):\n",
        "\n",
        "        # folder_name=self.root+'images'\n",
        "        filenames_before=self.filenames_before\n",
        "        print(\"before\",filenames_before.__len__())\n",
        "\n",
        "        if self.mode == \"train\":  # 80% for train\n",
        "            filenames_splitted =filenames_before[:int(0.8*filenames_before.__len__())]\n",
        "        elif self.mode == \"valid\":  # 10% for validation\n",
        "            filenames_splitted = filenames_before[int(0.8*filenames_before.__len__()):int(0.9*filenames_before.__len__())]\n",
        "        elif self.mode == \"test\":  # 10% for test\n",
        "            filenames_splitted = filenames_before[int(0.9*filenames_before.__len__()):]\n",
        "          \n",
        "        print(\"before adding extra files\",self.mode,int(filenames_splitted.__len__()))\n",
        "\n",
        "        new_img_dir = \"/content/drive/MyDrive/4-2/ML project/CVC-dataset/PNG_EXTRA/\"\n",
        "        images_directory = os.path.join(new_img_dir, \"images\")\n",
        "\n",
        "        png_extra_images = []\n",
        "        for images in os.listdir(images_directory):\n",
        "                \n",
        "                image_name = images.split(\"m\")[0]  # m from  \"m\"odifeid\n",
        "\n",
        "                if image_name in filenames_splitted and random.uniform(0, 1) <= 0.5:\n",
        "                   png_extra_images.append(images.split(\".\")[0])\n",
        "\n",
        "        # print(png_extra_images)  \n",
        "        for item in png_extra_images:\n",
        "           filenames_splitted.append(item)\n",
        "\n",
        "        print(\"final \",filenames_splitted)\n",
        "        print(\"after adding extra files\",self.mode,int(filenames_splitted.__len__()))\n",
        "        print(\"mode : \",self.mode)\n",
        "\n",
        "        temp = filenames_splitted.copy()\n",
        "        temp.sort()\n",
        "        print(temp)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # filenames = [x.split(\".\")[0] for x in filenames]\n",
        "        \n",
        "         # def oxford_split(self):\n",
        "        #     split_filename = \"test.txt\" if self.mode == \"test\" else \"trainval.txt\"\n",
        "        #     split_filepath = os.path.join(self.root, \"annotations\", split_filename)     # Truth text files from -> annotations/test.txt or annotations/trainval.txt\n",
        "        #     with open(split_filepath) as f:\n",
        "        #         split_data = f.read().strip(\"\\n\").split(\"\\n\")\n",
        "        #     filenames = [x.split(\" \")[0] for x in split_data]\n",
        "\n",
        "        #     # Splitting the train-validations\n",
        "        #     if self.mode == \"train\":  # 90% for train\n",
        "        #         filenames = [x for i, x in enumerate(filenames) if i % 10 != 0]\n",
        "        #     elif self.mode == \"valid\":  # 10% for validation\n",
        "        #         filenames = [x for i, x in enumerate(filenames) if i % 10 == 0]\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        TODOS\n",
        "            1. Extract all image names in self.root direct  if AcceleratorRegistry[acc_str][\"accelerator\"].is_available()ory (e.g. they have Abyssinian_100.png as the image, the filenames are like Abyssinian_100 without extensions)\n",
        "            2. Shuffle the images (Optional but recommended)\n",
        "            3. Allocate favorable portions to train-val-test. They have (3312, 368, 3669) for (train, val, test) from the same 7393 images, all disjoint\n",
        "            note : get all the names from the image directory. Then use this way:\n",
        "                    1. root + 'image' + 'img_name' for image\n",
        "                    2. root + 'annotation'+'img_name' for annotation\n",
        "\n",
        "            ** just refer to their test.txt and trainval.txt for any clarification or how they used the names\n",
        "            ** their split and extractions are commented below if needed\n",
        "        \"\"\"\n",
        "\n",
        "       \n",
        "        \"\"\".....DO THE TODOS HERE....\"\"\"\n",
        "\n",
        "        return filenames_splitted\n",
        "\n",
        "\n",
        "class SimpleCVCDataset(CVCDataset):\n",
        "    def __getitem__(self, *args, **kwargs):\n",
        "\n",
        "        sample = super().__getitem__(*args, **kwargs)\n",
        "\n",
        "        # resize images\n",
        "        image = np.array(Image.fromarray(sample[\"image\"]).resize((256, 256), Image.LINEAR))\n",
        "        mask = np.array(Image.fromarray(sample[\"mask\"]).resize((256, 256), Image.NEAREST))\n",
        "        # trimap = np.array(Image.fromarray(sample[\"trimap\"]).resize((256, 256), Image.NEAREST))\n",
        "\n",
        "        # convert to other format HWC -> CHW\n",
        "        sample[\"image\"] = np.moveaxis(image, -1, 0)\n",
        "        sample[\"mask\"] = np.expand_dims(mask, 0)\n",
        "        # sample[\"trimap\"] = np.expand_dims(trimap, 0)\n",
        "\n",
        "        return sample\n",
        "\n",
        "class TqdmUpTo(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ZcrwTeif47"
      },
      "outputs": [],
      "source": [
        "def transform_image(image,mask):\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=30)\n",
        "    ])\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    # transforms.RandomHorizontalFlip()\n",
        "    # ])\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    # transforms.RandomVerticalFlip()\n",
        "    # ])\n",
        "\n",
        "    # transform=transforms.Compose([\n",
        "    # transforms.RandomVerticalFlip(),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomRotation(30),\n",
        "    # transforms.RandomChoice([\n",
        "    # transforms.Pad(padding=10, padding_mode='reflect'),\n",
        "    # transforms.CenterCrop(480),\n",
        "    # transforms.RandomRotation(20),\n",
        "    # transforms.CenterCrop((576,432)),\n",
        "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    # ]),\n",
        "    # transforms.Resize((256,256)),\n",
        "    # transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    # ])\n",
        "   \n",
        " \n",
        "\n",
        "    return transform(image) , transform(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlLz68j1IxS1"
      },
      "outputs": [],
      "source": [
        "# root = \"/content/drive/MyDrive/4-2/ML project/CVC-dataset/PNG/\"\n",
        "\n",
        "# new_img_dir = \"/content/drive/MyDrive/4-2/ML project/CVC-dataset/PNG_EXTRA/images\"\n",
        "# new_msk_dir = \"/content/drive/MyDrive/4-2/ML project/CVC-dataset/PNG_EXTRA/annotations\"\n",
        "# os.makedirs(new_img_dir,exist_ok=True)\n",
        "# os.makedirs(new_msk_dir,exist_ok=True)\n",
        "\n",
        "# images_directory = os.path.join(root, \"images\")\n",
        "# masks_directory = os.path.join(root, \"annotations\")\n",
        "\n",
        "# for images in os.listdir(images_directory):\n",
        "        \n",
        "#         image_name = images.split(\".\")[0]\n",
        "#         # print(image_name)\n",
        "#         image_path = os.path.join(images_directory, images)\n",
        "#         mask_path = os.path.join(masks_directory,images)\n",
        "\n",
        "#         image = Image.open(image_path).convert(\"RGB\")\n",
        "#         mask  = Image.open(mask_path).convert(\"L\")\n",
        "       \n",
        "#         image , mask = transform_image(image,mask)\n",
        "\n",
        "#         image = image.save(f\"{new_img_dir}/{image_name}modified.png\")\n",
        "#         mask = mask.save(f\"{new_msk_dir}/{image_name}modified.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRwVkEbMSWHZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cvc_data_handler():\n",
        "    # root = \"Binary Segmentation_Cats\"   # *change the root directory\n",
        "    root = \"/content/drive/MyDrive/4-2/ML project/CVC-dataset/PNG/\"\n",
        "\n",
        "    # shuffle the contents\n",
        "\n",
        "    folder_name=root+'images'\n",
        "    filenames=os.listdir(folder_name)\n",
        "    filenames = [x.split(\".\")[0] for x in filenames]\n",
        "\n",
        "\n",
        "    random.shuffle(filenames)\n",
        "    \n",
        "    transform_train = None\n",
        "\n",
        "   \n",
        "    # init train, val, test sets\n",
        "    train_dataset = SimpleCVCDataset(root,filenames, \"train\",transform=transform_train)\n",
        "    valid_dataset = SimpleCVCDataset(root,filenames, \"valid\",transform=transform_train)\n",
        "    test_dataset = SimpleCVCDataset(root,filenames, \"test\",transform=transform_train)\n",
        "\n",
        "    # print(\"train_dataset = {}\".format(train_dataset.filenames))\n",
        "    # print(\"train_dataset = {}\".format(len(train_dataset.filenames)))\n",
        "    # print(\"valid_dataset = {}\".format(len(valid_dataset.filenames)))\n",
        "    # print(\"test_dataset = {}\".format(len(test_dataset.filenames)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # It is a good practice to check datasets don`t intersects with each other\n",
        "    assert set(test_dataset.filenames).isdisjoint(set(train_dataset.filenames))\n",
        "    assert set(test_dataset.filenames).isdisjoint(set(valid_dataset.filenames))\n",
        "    assert set(train_dataset.filenames).isdisjoint(set(valid_dataset.filenames))\n",
        "\n",
        "    print(f\"Train size: {len(train_dataset)}\")\n",
        "    print(f\"Valid size: {len(valid_dataset)}\")\n",
        "    print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "    n_cpu = os.cpu_count()\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=n_cpu)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader\n",
        "\n",
        "train_dataloader, valid_dataloader, test_dataloader = cvc_data_handler()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qyuw1YA5b7y"
      },
      "outputs": [],
      "source": [
        "# # # init train, val, test sets\n",
        "# root = '.'\n",
        "# train_dataset = SimpleOxfordPetDataset(root, \"train\")\n",
        "# valid_dataset = SimpleOxfordPetDataset(root, \"valid\")\n",
        "# test_dataset = SimpleOxfordPetDataset(root, \"test\")\n",
        "\n",
        "# # It is a good practice to check datasets don`t intersects with each other\n",
        "# assert set(test_dataset.filenames).isdisjoint(set(train_dataset.filenames))\n",
        "# assert set(test_dataset.filenames).isdisjoint(set(valid_dataset.filenames))\n",
        "# assert set(train_dataset.filenames).isdisjoint(set(valid_dataset.filenames))\n",
        "\n",
        "# print(f\"Train size: {len(train_dataset)}\")\n",
        "# print(f\"Valid size: {len(valid_dataset)}\")\n",
        "# print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "# n_cpu = os.cpu_count()\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=n_cpu)\n",
        "# valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)\n",
        "\n",
        "# print(type(train_dataset))\n",
        "# sample = train_dataset[0]\n",
        "# print(type(sample))\n",
        "# print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDIIs_NxwEHG"
      },
      "outputs": [],
      "source": [
        "# # for getting access to train_dataset , portion of code from  cvc_data_handler() is  extracted out here below\n",
        "# root = \"/content/drive/MyDrive/4-2/ML project/CVC-dataset/PNG/\"\n",
        "\n",
        "# # init train, val, test sets\n",
        "# train_dataset = SimpleCVCDataset(root, \"train\")\n",
        "# valid_dataset = SimpleCVCDataset(root, \"valid\")\n",
        "# test_dataset = SimpleCVCDataset(root, \"test\")\n",
        "\n",
        "# # print(\"train_dataset = {}\".format(train_dataset.filenames))\n",
        "# # print(\"train_dataset = {}\".format(len(train_dataset.filenames)))\n",
        "# # print(\"test_dataset = {}\".format(len(valid_dataset.filenames)))\n",
        "# # print(\"test_dataset = {}\".format(len(test_dataset.filenames)))\n",
        "\n",
        "# # It is a good practice to check datasets don`t intersects with each other\n",
        "# assert set(test_dataset.filenames).isdisjoint(set(train_dataset.filenames))\n",
        "# assert set(test_dataset.filenames).isdisjoint(set(valid_dataset.filenames))\n",
        "# assert set(train_dataset.filenames).isdisjoint(set(valid_dataset.filenames))\n",
        "\n",
        "# # print(f\"Train size: {len(train_dataset)}\")\n",
        "# # print(f\"Valid size: {len(valid_dataset)}\")\n",
        "# # print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "# n_cpu = os.cpu_count()\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=n_cpu)\n",
        "# valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=n_cpu)\n",
        "\n",
        "# print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQdeb_mGwzoX"
      },
      "outputs": [],
      "source": [
        "# print(type(train_dataset))\n",
        "# sample = train_dataset.__getitem__(0)\n",
        "# print(sum(sum(sum((sample[\"mask\"]<255)>0))))\n",
        "\n",
        "\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.imshow(sample[\"image\"].transpose(1, 2, 0)) # for visualization we have to transpose back to HWC\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.imshow(sample[\"mask\"].squeeze())  # for visualization we have to remove 3rd dimension of mask\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4nq08ILaYhn"
      },
      "outputs": [],
      "source": [
        "# # # lets look at some samples\n",
        "\n",
        "# sample = train_dataset[0]\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.imshow(sample[\"image\"].transpose(1, 2, 0)) # for visualization we have to transpose back to HWC\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.imshow(sample[\"mask\"].squeeze())  # for visualization we have to remove 3rd dimension of mask\n",
        "# plt.show()\n",
        "\n",
        "# sample = valid_dataset[0]\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.imshow(sample[\"image\"].transpose(1, 2, 0)) # for visualization we have to transpose back to HWC\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.imshow(sample[\"mask\"].squeeze())  # for visualization we have to remove 3rd dimension of mask\n",
        "# plt.show()\n",
        "\n",
        "# sample = test_dataset[0]\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.imshow(sample[\"image\"].transpose(1, 2, 0)) # for visualization we have to transpose back to HWC\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.imshow(sample[\"mask\"].squeeze())  # for visualization we have to remove 3rd dimension of mask\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg4_bxKV5BaQ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvKlqPH6sKtz"
      },
      "outputs": [],
      "source": [
        "# model = PetModel(\"FPN\", \"resnet34\", in_channels=3, out_classes=1)\n",
        "# trainer = pl.Trainer(\n",
        "#     gpus=0, \n",
        "#     max_epochs=10,\n",
        "# )\n",
        "\n",
        "# trainer.fit(\n",
        "#     model, \n",
        "#     train_dataloaders=train_dataloader, \n",
        "#     val_dataloaders=valid_dataloader,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeGCIYNlVx5y"
      },
      "outputs": [],
      "source": [
        "class PetModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = smp.create_model(\n",
        "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
        "        )\n",
        "\n",
        "        # preprocessing parameteres for image\n",
        "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
        "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
        "\n",
        "        # for image segmentation dice loss could be the best first choice\n",
        "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
        "\n",
        "    def forward(self, image):\n",
        "        # normalize image here\n",
        "        image = (image - self.mean) / self.std\n",
        "        mask = self.model(image)\n",
        "        return mask\n",
        "\n",
        "    def shared_step(self, batch, stage):\n",
        "        \n",
        "        image = batch[\"image\"]\n",
        "\n",
        "        # Shape of the image should be (batch_size, num_channels, height, width)\n",
        "        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n",
        "        assert image.ndim == 4\n",
        "\n",
        "        # Check that image dimensions are divisible by 32, \n",
        "        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n",
        "        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n",
        "        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n",
        "        # and we will get an error trying to concat these features\n",
        "        h, w = image.shape[2:]\n",
        "        assert h % 32 == 0 and w % 32 == 0\n",
        "\n",
        "        mask = batch[\"mask\"]\n",
        "\n",
        "        # Shape of the mask should be [batch_size, num_classes, height, width]\n",
        "        # for binary segmentation num_classes = 1\n",
        "        # print(\"mask shape\", mask.shape)\n",
        "        # mask=mask.reshape((mask.shape[0],1,mask.shape[2],mask.shape[3]))\n",
        "        # mask=mask.permute(0,3,1,2)\n",
        "        # print(\"mask shape\", mask.shape)\n",
        "        # print(\"image shape\", image.shape)\n",
        "        assert mask.ndim == 4\n",
        "\n",
        "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
        "        mask=mask/255\n",
        "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
        "        # print(\"mask val passed\")\n",
        "\n",
        "        logits_mask = self.forward(image)\n",
        "        # print(\"forward done\")\n",
        "        # print(\"logits_mask\",logits_mask.shape)\n",
        "        # print(\"mask\",mask.shape)\n",
        "        \n",
        "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
        "        loss = self.loss_fn(logits_mask, mask)\n",
        "\n",
        "        # Lets compute metrics for some threshold\n",
        "        # first convert mask values to probabilities, then \n",
        "        # apply thresholding\n",
        "        # print(\"prob_mask in\")\n",
        "        prob_mask = logits_mask.sigmoid()\n",
        "        # print(\"pred_mask in\")\n",
        "\n",
        "        pred_mask = (prob_mask > 0.5).float()\n",
        "\n",
        "        # We will compute IoU metric by two ways\n",
        "        #   1. dataset-wise\n",
        "        #   2. image-wise\n",
        "        # but for now we just compute true positive, false positive, false negative and\n",
        "        # true negative 'pixels' for each image and class\n",
        "        # these values will be aggregated in the end of an epoch\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
        "        # print(\"shared step done\")\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"tp\": tp,\n",
        "            \"fp\": fp,\n",
        "            \"fn\": fn,\n",
        "            \"tn\": tn,\n",
        "        }\n",
        "\n",
        "    def shared_epoch_end(self, outputs, stage):\n",
        "        # aggregate step metics\n",
        "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
        "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
        "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
        "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
        "\n",
        "        # per image IoU means that we first calculate IoU score for each image \n",
        "        # and then compute mean over these scores\n",
        "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
        "        \n",
        "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
        "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
        "        # in this particular case will not be much, however for dataset \n",
        "        # with \"empty\" images (images without target class) a large gap could be observed. \n",
        "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
        "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "        \n",
        "        # prec = tp / (tp+fp)\n",
        "        # recall = tp / (tp+fn)\n",
        "        # f1 = 2*prec*recall / (prec+recall)\n",
        "\n",
        "        metrics = {\n",
        "            f\"{stage}_per_image_iou\": per_image_iou,\n",
        "            f\"{stage}_dataset_iou\": dataset_iou,\n",
        "        }\n",
        "\n",
        "        \n",
        "        self.log_dict(metrics, prog_bar=True)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, \"train\")            \n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        return self.shared_epoch_end(outputs, \"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, \"valid\")\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        return self.shared_epoch_end(outputs, \"valid\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, \"test\")  \n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        return self.shared_epoch_end(outputs, \"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uu8YXPh7lh_B"
      },
      "outputs": [],
      "source": [
        "model = PetModel(\"FPN\", \"resnet34\", in_channels=3, out_classes=1)\n",
        "trainer = pl.Trainer(\n",
        "    gpus=0, \n",
        "    max_epochs=10,\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    model, \n",
        "    train_dataloaders=train_dataloader, \n",
        "    val_dataloaders=valid_dataloader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-YUI8oH-sfL"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eP2irt8-CU4"
      },
      "outputs": [],
      "source": [
        "model_path =\"/content/drive/MyDrive/4-2/ML project/CVC-dataset/models/\"\n",
        "# os.makedirs(model_path,exist_ok=True)\n",
        "import pickle\n",
        "counter = 3\n",
        "with open(model_path+'model_'+str(counter)+'.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFmMfqSe3tv3"
      },
      "source": [
        "## Validation and test metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM4B8hSvy1kl"
      },
      "outputs": [],
      "source": [
        "# run validation dataset\n",
        "valid_metrics = trainer.validate(model, dataloaders=valid_dataloader, verbose=False)\n",
        "pprint(valid_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V55AyHW4LKg"
      },
      "outputs": [],
      "source": [
        "# run test dataset\n",
        "test_metrics = trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
        "pprint(test_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H5oTdUc3hb9"
      },
      "source": [
        "# Result visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CUYlGTp00Fb"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(test_dataloader))\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    logits = model(batch[\"image\"])\n",
        "pr_masks = logits.sigmoid()\n",
        "\n",
        "for image, gt_mask, pr_mask in zip(batch[\"image\"], batch[\"mask\"], pr_masks):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image.numpy().transpose(1, 2, 0))  # convert CHW -> HWC\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(gt_mask.numpy().squeeze()) # just squeeze classes dim, because we have only one class\n",
        "    plt.title(\"Ground truth\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(pr_mask.numpy().squeeze()) # just squeeze classes dim, because we have only one class\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}